---
title: "Exploring Data Science Education: From Tutorials to Assessment"
subtitle: "Duke Statistical Science | Graduation with Distinction"
date: 2023-04-11
date-format: long
author: 
    - Evan Dragich
    - supervised by Mine Ã‡etinkaya-Rundel, PhD.
format:
  revealjs: 
    slide-number: true
    incremental: true
    chalkboard: 
      buttons: true
    preview-links: auto
    logo: images/dukelogo.png
    css: presentation.css
    footer: <https://stat.duke.edu>
---

## Introduction/ About Me

-   Basic demographic background information
-   How i found myself in Duke StatSci (leads into)
-   How i found myself working on this thesis

## Agenda/Thesis TOC

-   explanation of the 2 strands
-   summary of following slides (and stuff like there will be time for
    questions, etc.)

# Building a Data Science Assessment

## Background

-   Colloquially motivate the need for a DS concept inventory
    -   CAOS
-   Data Science as it emerges as a field--what is it, exactly?
-   How exactly do people: (1) make, (2) pilot, (3) validate new concept
    inventories or scales?

## Inital cleaning/getting my feet wet

-   Should i even present on this? Is any of it interesting/worth diving
    into?

## Interviews

-   reminder of what we tried to do (3 faculty to see if scope was
    appropriate from instructor perspective, 3 TA to see if questions
    were landing on a closer-to-target population, but still with some
    DS context)
-   Summarize results from faculty, and themes (mainly that you could
    clearly tell the CS prof from the two stats ones via what they
    thought should be included, things pointed out like uncomfortable
    contexts (storm paths?))

## Case Study: Application Screening {.smaller}

start with AS: a question based on proxy variable.

*You are working on a team that is making a deterministic model to
quickly screen through applications for a new position at the company.
Based on employment laws, your model may not include variables such as
age, race, and gender, which could be potentially discriminatory.*

*Your colleague suggests including a rule that eliminates candidates
with more than 20 years of previous work experience, because they may
have high salary expectations. Why might using this variable be
considered unethical? Explain your answer.*

Oops, best practices would phrase this in a non-leading way. If a
student wasn't initially going to think this would be unethical, but we
told them it might be somehow, their explanation won't be as valuable as
someone who would have answered right away. Okay, let's rephrase to make
them answer whether it is or isnt and

## Case Study: Application Screening {.smaller}

*You are working on a team that is making a deterministic model to
quickly screen through applications for a new position at the company.
Based on employment laws, your model may not include variables such as
age, race, and gender, which could be potentially discriminatory.*

*Your colleague suggests including a rule that eliminates candidates
with more than 20 years of previous work experience, because they may
have high salary expectations. Are there ethical implications of using
this variable to select candidates? Explain your answer.*

Well... that doesn't help much. We still have the classic selection bias
clouding results; students would think "well, if there wasn't an ethical
problem, they wouldnt have included this as one of the only ethics
questions on the assessment." Plus, how are we grading this? What are we
looking for to confirm that they understand the proxy variable? It might
work to set up an autograder that marks "correct" if they mark "yes" to
the ethics question AND mention "proxy" in their response. But, this is
an introductory-level assessment. Will students be able to concisely
describe employment expeirence as a "proxy," or would explanations be
wordier and might include "is correlated with," "is related to," "goes
hand in hand with." If we want autogradable, looking like MC is going to
be main way to go. Note that, at this stage, Employment Screening and
several other similar questions are left in open-ended format.

## Case Study: Data Confidentiality {.smaller}

so, after that whole mess, a general conclusion is that MC might be the
only way to go. so how do we write a good MC ethics question? here's a
start

*A newspaper reports on the results of a survey from a small (\<2000
student) college The college agrees to have the data released to the
public so long as the students' identities and academic standing
information are kept confidential. Which of the following combinations
of variables is less likely to unintentionally identify any students?
Explain.*

*a. Year, major, sports played*

*b. Year, major*

Well, first of all, is "college" the best word here? While it's roughly
synonymous with "university" in the US, they can have very different
meanings country-to-country. Thus, let's eliminate any ambiguity right
off the bat.

## Case Study: Data Confidentiality {.smaller}

*A newspaper reports on the results of a survey from a small (\<2000
student) university. The university agrees to have the data released to
the public so long as the students' identities and academic standing
information are kept confidential. Which of the following combinations
of variables is less likely to unintentionally identify any students?
Explain.*

*a. Year, major, sports played*

*b. Year, major*

Great! There is no issue with grading this on a large scale, as students
will simply choose option "a" to be marked correct. But, how valid is
this binary comparison of two nearly-identical options in measuring
students' idea of data privacy (and key variables whose intersections
can quickly narrow down populations). Would they choose "a" simply
because of the "presence vs absence" heuristic, similar to the selection
bias issue addressed earlier, or because they understand how it quickly
narrows down who a respondent could be? This question took a lot of
brainstorming and workshopping, and we ultimately landed on the
following options:

*a. Class year and sports played*

*b. Student ID and dorm zip code*

*c. GPA and major*

*d. Birth date and phone number*

*e. None of the above*

::: notes
I really like this current version for a few reasons. First, "none of
the above." It doesn't solve everything, but the closest you can get to
having an open-ended prompt in a true MC assignment is to have a "none
of the above," and to use it! (roughly proportionally, so \~20-25% of
the time). Second, the options are well-distributed and take some time
to think through. We refined this between faculty and student
interviews, and all three students had a similar response. A nice rhythm
as they read through the responses, respectively "hmm, definitely not,
ummmm maybe?, yeah definitely not, uh". Immediately eliminating "b" and
"d", students debated "a" and "c" on a metric we hadn't even surmised
when writing. "Well, in terms of figuring out who people are, class year
and sports played would do that. BUT, knowing GPA and major wouldn't
really let you identify anyone, but GPA is more sensitive data than
class year/sports played in that it would be more harmful if released,
and linked back to the people." Out of all the think-aloud interviews,
this single moment stood out as justifying why we asked students to walk
through their reasoning processes, and to "think out loud:" to catch
unexpected thought processes like this.
:::

## Assessment Next Steps

-   199 Pilot
-   IRB Roadblocks
-   NSF Grant?
    -   Turn into more robust JS framework like CAOS is

# Working on the `dsbox` package

## `dsbox` package {.smaller}

-   Reference growing DS interest and scalability of education from the
    assessment talk earlier; that plus the opensource nature of R lends
    naturally to making such a standardized curriculum

-   What is Data Science in a Box? Its that \^. Using `tidyverse` to
    practice basic data wrangling, visualization, and modeling.

-   That curriculum set was then condensed into a package for
    self-learners called `dsbox`, which users can download and follow to
    become well-acquainted with basic data science in R.

## How does it work? {.smaller}

-   2 key packages: `learnr` and `gradethis`.

-   `learnr` provides a robust framework for turning RMarkdown documents
    into interactive tutorials, where users can be guided through
    running and writing code, quiz questions, watching videos, etc,
    directly in the "Tutorial" pane in RStudio. A key feature is that
    progress is saved, so you can resume working in RStudio whenever.

-   `gradethis` takes that basic, broad framework, and provides tools
    for drilling down deeper when grading. Instructors can provide
    feedback for a variety of common mistakes with sophisticated testing
    logic.

## Creating a Tutorial

-   9 existing, skeleton for 1 (these corresponded to all HWs from
    DSinaBox; the package had skeleton .Rd files for the dataset
    already).

-   Decided I would try to recreate that tutorial, adding in some flair
    and thoughts based on best practices

    -   scaffolded one of the exercises more to match the "interactive
        tutorial" modality vs the "take home, class homework assignment"

## Releasing to CRAN

-   Explaining what CRAN is

-   Explaining what the DESCRIPTION folder and dependencies are

-   Unfortunately, `gradethis` is still in development and thus not yet
    released on CRAN itself. In turn, we are unable to upload a package
    that specifies a package not on CRAN to be one of its dependencies.
    We have submitted an issue

# Discussion

## Learning Takeaways

-   Learned advanced computing I wouldn't have gotten otherwise in my
    abbreviated trip through the Stat major

-   Learned how to interact with others' code beyond scope of
    classroom/research team (making and reviewing public PR requests,
    compiling and standardizing and revising the assessment and package)

## Reflections

-   Statement that "teaching material is only way to master it" had
    always been true for my tutoring and TAing experiences; developing
    and studying to the point of scrutiny a curriculum took my
    understanding to the next level

-   Newfound appreciation for work that has gone into all the
    educational curriculum materials today, tools like `ghclass` and
    `learnr`, all packages that have been released and are maintained
    and what it takes to do that.

-   Inspired me to continue interacting with the world of open source
    software even though my job (for the meantime) is to be an Excel
    monkey for 40 hours a week

## Q&A

-   Display contact information/Linkedin on screen for this?
