[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Data Science Education: From Tutorials to Assessment",
    "section": "",
    "text": "Abstract\nAs data science continues to grow in popularity among university course offerings, it is becoming crucial to successfully measure students’ learning outcomes in introductory courses. To do this requires an assessment which could additionally be used to evaluate pedagogical techniques or curriculum interventions in data science courses.\nTo develop a blueprint for the assessment, a multi-institutional team of statistics and data science education researchers identified common data science content (e.g., data wrangling, interpreting visualizations), drawing from published guidelines and recommendations as well as introductory data science syllabi. A draft of the assessment was written and used to conduct three think-aloud interviews with field-relevant faculty members. The interviews consisted of both open-ended brainstorming on the assessment’s scope as well as individual examinations of each item for relevance, clarity, and efficacy in measuring the desired learning objective. Think-aloud interviews were also conducted with TAs of an introductory DS course to gauge item clarity and gain insight into the reasoning for their responses.\nAs well, given the recent rise in popularity of open source educational software, there is a growing demand for scalable data science pedagogical materials. Based on the Data Science in a Box introductory curriculum, we have developed an R package dsbox containing 10 interactive, self-contained, auto-graded tutorials covering concepts from basic data wrangling and visualization to modeling.\nThis work includes descriptions of the blueprint developed for both the assessment and package, as well as examples of assessment items and tutorials, and results from the faculty and student think aloud interviews. We also present next steps for the project including plans for larger scale piloting and further analyses.\n\n\nGoals\nBy sharing the work, we hope that instructors will become familiar with an assessment they may use for designing introductory data science curriculum or researching classroom innovations. We also hope that this instrument can serve as an inspiration or a starting point to be tailored by future researchers more specifically to their courses or to another discipline (e.g. by adding more programming concepts to better serve a more computing-focused introductory data science class, etc.) We hope to officially release dsbox to CRAN by the end of the Spring 2023 semester, facilitating access to data science education to a broader community.\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In 2020, group of multi-institutional researchers recognized a gap in the data science education field for a validated introductory-level assessment, mirroring those for other more concretely-defined disciplines. Inspired by the CAOS project, which in 2005 produced a 40-item introductory statistics inventory, the team sought to develop an analog for data science concepts, modeled after CAOS’s balance of a broad, relevant scope with accessibility to learners with a variety of backgrounds and prerequisite knowledge. To create an assessment that would be appropriate to administer both as a pre-test at the beginning of an introductory course as well as a summative post-test, the process of writing items required care in the wording and presentation of nuanced, process-oriented topics such as ethics and data wrangling to accurately measure a student’s knowledge of those topics via a series of multiple choice questions. As data science programs’ curricula vary across institutions and even across departments at the same institution, this became a non-trivial task, as it first required a synthesis of which topics generally constitute the learning objectives of an introductory data science course.\nWhile the rest of the team of researchers began reviewing curricula and drafting items earlier, I joined the project in January 2022 as an independent study student. As well, the rest of the group will continue to work on the assessment as it continues to evolve after my graduation from Duke in May 2023. Thus, the work presented in this document reflects only a portion of the life cycle of the data science assessment, through the lens of my contributions as an undergraduate student. The Background is a traditional literature review to motivate such a data science concept inventory assessment. The two Development sections detail my contributions and reflections while developing the assessment and the package respectively. The Discussion synthesizes the previous sections and reflects on my educational experience while looking ahead to the next steps for the projects. Finally, the Appendices include a prototype of the assessment at the time of writing, the set of ultimately discarded passages and items, and screenshots displaying the dsbox package in use.\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Assessment Background",
    "section": "",
    "text": "Background\nAn essential component of any educational research is a validated, relevant instrument to measure students’ learning outcomes. Whether to award college credit like the College Board’s AP and CLEP exams, to gauge students’ background knowledge at the start of a course, or just as a tool upon which to evaluate educational interventions, such assessments have been developed and statistically analyzed for a wide range of subjects such as Spanish, Psychology, Chemistry, and Calculus (Godfrey and Jagesic 2016; Solomon et al. 2021; Mulford and Robinson 2002; Epstein 2013).\nIn the field of statistics, previous work on measuring students’ reasoning skills led to the development of the Comprehensive Assessment of Outcomes in Statistics (CAOS). The revised CAOS 4, comprising 40 multiple-choice items on a variety of commonly-taught first-semester introductory concepts, was first administered in 2005 and allowed instructors to measure whether their courses were successfully resulting in their desired learning outcomes. However, many instructors noted that their findings reflected a much lower understanding than expected, specifically regarding the topics of data visualization and data collection (Delmas et al. 2007). Notably, a key feature of the CAOS was the lack of hard computation nor need to recall specific formulas or definitions, allowing greater accessibility for a variety of statistics-adjacent uses. In fact, while initially motivated for research instrument purposes, early pilots of the CAOS found that instructors used the assessment results “for a variety of purposes, namely, to assign a grade in the course, for review before a course exam, or to assign extra credit” (Delmas et al. 2007).\nIn 2023, as more high schools and universities continue to support the emerging field of data science via specific courses, concentrations, or even majors, there is a need to measure students’ learning outcomes in these introductory classes analogously to the subjects named above (Swanstrom, n.d.; Schanzer et al. 2022). Lacking a clearly-defined scope, empirical studies of so called “data science” curricula suggest that the field can be thought of as an augmentation of traditional statistical modeling concepts, with emphases on computing, data visualization and manipulation, as well as a consideration of ethics and the role that data plays in society (Zhang and Zhang 2021).\nSpecifically, a review of five introductory data science courses found that, while choice of language varied, all curricula involved some amount of computing or pseudocode (Çetinkaya-Rundel and Ellison 2021). The next highest frequency topics among curricula were inference and modeling, closely followed by data visualization and data wrangling, with most courses also having some smaller component of communication and ethics. This empirical set of topics corroborates theoretical results from an earlier, larger conference of 25 data science-adjacent faculty, which identified six key competencies for undergraduate data science majors–computational thinking, mathematical foundations, model building and assessment, algorithms and software foundation, data curation, and communication (De Veaux et al. 2017).\nThus motivates the need for a language-agnostic, broad-scope data science assessment that can be tailored further to best meet the needs of specific programs. Given the breadth of diversity captured in the five curricula outlined in Çetinkaya-Rundel and Ellison (2021), collaborating with a group of data science faculty to write such an assessment allows for a wide array of subjects to be covered, while still letting each member develop questions based on material they personally teach. While this still does not guarantee that all possible concepts from an introductory data science course would be covered in such an assessment, the goal then becomes achieving saturation within themes from faculty feedback; there should be no single topic identified across faculty as missing from a comprehensive assessment, even though individual perceptions of coverage may vary (Delmas et al. 2007). In order to let items best measure students’ thinking processes, think-aloud interviews with students are essential, not just to clarify potentially confusing wording, but also to ensure that students respond to each item via the thought process intended by the researchers (Reinhart et al. 2022). Previous work has found that concept inventory-style assessments, while succeeding in measuring students’ overall mastery of a topic, fail to specifically measure students’ inaccurate perceptions. Thus, a sound assessment should result from observing misconceptions during think-aloud interviews and arriving at a series of questions that target broader learning objectives rather than specific misconceptions (Jorion, Gane, James, et al. 2015).\nIn addition to measuring students’ overall data science mastery, the breadth of topics covered lends naturally to the development of subscales, or subsets of the overall scale’s items from which a student’s subscore can be calculated for a particular topic. Common methods include “subscale alphas, exploratory factor analysis, and confirmatory factor analysis” (Jorion, Gane, DiBello, et al. 2015). While the present focus of the work is the early stages of item creation, iteration, and think-aloud interviewing, keeping track of crucial opportunities to administer a pilot assessment to a class on a large-scale must be carefully monitored, given the limited opportunities to do so per semester (Study et al. 2018).\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "development.html#phase-0-initial-cleaning-and-feedback",
    "href": "development.html#phase-0-initial-cleaning-and-feedback",
    "title": "Assessment Development",
    "section": "Phase 0: Initial Cleaning and Feedback",
    "text": "Phase 0: Initial Cleaning and Feedback\nIn January 2022, I inherited a GitHub repository with several documents: many were background information on what topics would be included in the assessment, with one containing all currently-written questions at various stages of completion. These questions were not yet organized into specific groups of stem and items, many sections were commented out or overwritten, and it was clearly something that had been written piece-wise by a group of people. My first true task was to run through the current questions myself, answer them how I would in an assessment-like context, and provide feedback on clarity, wording, and reflect on the topic covered.\nFrom this initial feedback, I created three pull requests attempting to improve some of these concerns. These first fixes were minorly substantial edits: changing “most” to “the majority of” to make a question less ambiguous, adding a “fill-in-the-blank” slot to make even more clear what a question is asking, and background information on confidence intervals to Births per Day (interpret visualization; summary statistics). The first two were approved by the rest of the team, but they rejected the third, asserting that the question stem already provided sufficient motivation on confidence intervals.\nI also cleaned up any “obvious” fixes from my initial review. These “no-brainer” edits were mainly typos, distorted or obscured plots, and items that are phrased in a way that don’t actually pose a question. It was also time to clean up the current document by converting it to a Quarto book, which would allow for easy webpage-like navigation. This marked my first exposure to Quarto, and I quickly grew to love its improvements to the RMarkdown workflow, like ease of rendering to different formats and intuitive structure of the index and YAML files. This was also when I received my first moment of creative liberty with the project, as I was tasked with dividing the ungrouped set of questions into discrete passages consisting of a stem and one or more corresponding items. Each of these passages—26 at the time—was given a title to identify it in the Quarto book sidebar.\nThe final step before presenting the assessment externally was to improve reproducibility, as many figures were not being rendered with each update, but rather embedded as images. I used the current images as guides to recreate a map of the US colored by region, movie-themed data tables, and pseudocode chunks. I had some slight HTML and CSS exposure in previous classes, but this new challenge of matching an existing format allowed me to expand my knowledge about web layout and styles.\nBy April 2022, concluding my first semester working on the project, we had a polished, reproducible, website-hosted prototype of the assessment ready to present externally for feedback."
  },
  {
    "objectID": "development.html#phase-1-faculty-interviews",
    "href": "development.html#phase-1-faculty-interviews",
    "title": "Assessment Development",
    "section": "Phase 1: Faculty Interviews",
    "text": "Phase 1: Faculty Interviews\nThe remainder of the time spent working on the assessment—April 2022 to February 2023—was spent gathering feedback via interviews, iteratively updating items, and removing weaker items identified in group discussions. We conducted a series of three think-aloud informational interviews with various faculty nationwide who teach or have taught introductory data science courses. Participants were recruited from a shortlist of Dr. Çetinkaya-Rundel’s data science education contacts, and were specifically chosen to represent a breadth of data science curricula. The three interviewees chosen were, in order of interview:\n\n\n\n\n\n\n\n\n\nName\nDepartment Affiliation\nInstitution Type\nField of Ph.D. Dissertation\n\n\n\n\nProfessor X\nStatistics\nLiberal arts college\nBiostatistics\n\n\nProfessor Y\nComputer Science\nR1 research university\nComputer science education\n\n\nProfessor Z\nComputer and Information Science\nLiberal arts school within university\nStatistics\n\n\n\nEach interview was scheduled for two hours long and consisted of three sections: open-ended introductory and concluding discussions, sandwiching an item-by-item run through of the assessment. In the initial discussion, we asked participants two big-picture questions: What topics must be in an introductory data science course? And, what topics are nice to have in an introductory data science course? For each item in the assessment run-through, we asked interviewees to narrate out loud their thinking process from start to finish: any initial reactions, their process to arrive at an answer, and what that answer would be. We then asked for additional comments or suggestions, ranging from small- (formatting changes) to large-scale (removing the question entirely). We then concluded with three big-picture questions: What are the strengths of the current assessment? What topics are missing from the current assessment? And, what is in the current assessment, but doesn’t belong?\nWhile I scheduled and directed the flow of the interviews myself, I was joined by Dr. Çetinkaya-Rundel for all three, Dr. Legacy for the second two, and Dr. Beckman for the third. Silently observing, these team members took notes in real-time while I was conversing with the interviewee as a supplement to the transcript. Following the interviews, I spot-checked the accuracy of the automatic transcription, focusing on moments where I remembered the interviewee offered a powerful insight. I then augmented the interview notes with such quotes and clarified where I could while the conversation was still fresh in my mind. Ultimately, I hoped to leave a clear record of the following crucial information for our team discussions: what the interviewees selected as a response to each item, whether their reasoning for said response differed from our intention, and whether they would include or omit each item.\n\nDiscipline-Specific Perspectives\nAfter conducting each of three interviews, we met to discuss the new feedback and, when appropriate, made modifications and deletions to the current assessment. While each interviewee came from distinct backgrounds, there were some salient themes about their perspectives on introductory data science that emerged through patterns in their responses.\nProfessor X (Statistics) seemed to have a somewhat similar perspective on “what is data science” as the research team—chiefly, visualization and wrangling. He noted positively when items featured multivariate data, particularly when interpreting visualizations. He also notably brought a wealth of pedagogical experience with real-life data to our consideration: such as that movie budgets and revenue display less compelling of a relationship than one would think, that movie-themed data in general is less relevant to younger generations, and that county data is generally too heterogeneous to be useful in most contexts. He also grounded our conceptions of what a pre-test student would know by claiming that residuals are now part of the Common Core in K-12 education. However, he thought residual analysis and other related modeling topics like supervised learning didn’t align with his experience teaching introductory data science. But, as in the case of Realty Tree (regression tree), he acknowledged that some topics may fall outside the scope of an introductory class, but could be reasoned through by an introductory student and provide them motivation to learn more.\nProfessor Z (Statistics and Information Science) held similar opinions on the focus of data science as did Professor X—visualization and wrangling. The main takeaway from her feedback was that we needed to consider carefully the scenarios posed in question stems. Hurricane data, like in Storm Paths (simulated data; interpret uncertainty), she claimed, may provoke discomfort in students from hurricane-prone areas. She noted that the former wording of He Said She Said (interpret visualization), in which the verbs in the items’ writing were in the present tense while those on the plot itself were in the past tenset, may present a burden to non-native English speakers. The SAT, referenced in Banana Conclusions (causation; statistical communication) may need more explanation for international students who are less familiar with the US admissions process. She also, like Professor X, thought that some of our more technical questions breached the scope of introductory data science, such as residual analysis and the distinction between training and validation sets. She was similarly a fan of Realty Tree (regression tree). However, while Professor X was in favor of summary statistic questions and mapping them to graphs, Professor Z called out language like “margin of error” in Births per Day (interpret visualization; summary statistics) as too statistical, and less appropriate here.\nAs a computer science education representative, we knew Professor Y’s interview would offer a unique perspective. This was evident from the start, when her first response to my question of “what is essential” was to ask me whether this course assumed prior coding experience or not. After I clarified that we are assuming no coding experience, she answered that learning coding “in the order that matters for data science”, e.g. functions first, is a primary topic, as is some understanding of transforming and cleaning data. But, she qualified, not too much as much cleaning can be done in advance for students in an introductory class. It wasn’t until the “what is nice to have” question that she mentioned data visualization. This CS-focused perspective continued when she pointed out that our pseudocode didn’t specify which type of join would be performed, that basic English vocabulary like “filter” and “select” might not be known in their data wrangling context, and generally dissuaded us from using pseudocode on something that could be a pre-test. Another notable pattern was that some of her suggested edits went against data visualization best practices. She suggested we change from a rainbow color scheme to a gradient one for the categorical data in Bikes and Scooters 1, and to reorder the y-axis in He Said She Said (interpret visualization) to be in alphabetical rather than sorted order. Finally, she led us to remove the logarithmic transformation present in Movie Budgets 1 (compare summary statistics visually) and all related questions, explaining that since the transformation itself isn’t relevant to the desired learning outcome, students seeing those words continually repeated may start losing sight of the item’s objective.\n\n\nRegrouping to synthesize feedback\nWhile the team met once between each faculty interview to make any minor changes before the next one, the bulk of the revision took place after all were completed, in the Fall 2022 semester. A central theme from all interviews was that our questions designed to test tricky, nuanced concepts (e.g. reading in data to statistical software) in a multiple choice format weren’t landing as we hoped. While we thought we had written a former item, Data Cleaning (computing with data; numerical reasoning), with enough language agnosticism, it ended up being too R-specific and baffling to faculty. To this item, and several others, Professor Z summed up the crux of our dilemma well, paraphrasing: “hmm.. I see what concept you’re getting at, but this question doesn’t really get there… Okay, well, this is something that’s hard to write to be auto-gradable but also actually measure. I don’t know how I would fix this but I really like the underlying idea.”\nEncouraged to keep these concepts on the assessment with modifications, we found it difficult to cut out any of the ~50 pilot questions we had at this point. One notable large-scale fix was combining the concepts tested on three former wrangling passages (Park Wrangling (pseudocode; data wrangling), Shopping Wrangling (data wrangling; column-wise operations), and TV Show Wrangling (data wrangling; joins)) into a single context, hoping to further reduce students’ cognitive load. We knew we needed to cut items down to the ~30 range, but found it very difficult to identify those items that were bringing the least to the current assessment. Interestingly, this involved almost no large disagreements between team members—there were no particularly polarizing items, like we observed during the faculty interviews—rather, we all genuinely couldn’t identify any items we wanted to cut.\nWe did eventually make rounds of sacrificial cuts, choosing to prioritize the culling of items that even slightly overlapped each other, and those that were the most straightforward. We acknowledged the trade-off that this may lead to students taking the assessment in a pre-test context unable to confidently answer a majority of the questions. However, we were constrained by the need for an instrument that could reasonably be given in an hour, and cutting out several entire passages was the only way forward. Nevertheless, there were a handful of similar items that we decided to keep for the student interviews, such as two pseudocode chunks (in Movie Wrangling (pseudocode; column-wise operations; joins)) that varied by a single statement. Here, we were ambivalent on which would be more effective, and hoped to let student feedback dictate whether one “stuck” better."
  },
  {
    "objectID": "development.html#phase-2-student-interviews",
    "href": "development.html#phase-2-student-interviews",
    "title": "Assessment Development",
    "section": "Phase 2: Student Interviews",
    "text": "Phase 2: Student Interviews\nThe chief purpose of Phase 2 was to see if the topics covered would be at the appropriate level for students with data science exposure, as well as to continue to refine wording and pacing. Having previously discussed “what is data science” with faculty, a similar series of interviews with Duke Statistical Science student teaching assistants (TAs) allowed us to start drilling down and seeing if items landed the way we thought they would.\nTo reflect this difference in the type of feedback being solicited, we omitted the initial broader scope questions when interviewing TAs. Combined with the fact that that the assessment was now about half its original length, these interviews were only one hour long. The final portion of big picture questions was kept, though, with slightly modified prompts: Are the pacing and length appropriate? Based on what you remember learning in intro data science, what topics are missing from the current assessment? Based on what you remember learning in intro data science, what is in the current assessment, but doesn’t belong?\nWe first reached out in November 2022 to all TAs at that time for STA 199 or STA 198, its health-themed analog. However, we were unable to recruit any students so close to finals, and ended up reaching out to the same group in January. The interviews were conducted in early February, thus consisting of students who were at least TAs for STA199 in the previous Fall semester. The three TAs recruited were, in order of interview:\n\n\n\n\n\n\n\n\nName\nDegree Year and Level\nProgram\n\n\n\n\nTA A\n2nd year Masters\nStatistical Science\n\n\nTA B\n4th year Undergraduate\nEconomics major, Statistical Science minor\n\n\nTA C\n1st year Masters\nStatistical Science\n\n\n\nIn general, while still engaging well and demonstrating a strong command of data science skills, the TAs tended to be much less verbose in their feedback. Across all interviews, a former item on Realty Tree (regression tree) that asked students to trace a non-trivial regression tree four separate times stood out as an interruption to the otherwise smooth flow of the assessment. Another common trend across TA interviews was the suggestion to modify plots’ themes for clarity–this surprised me, as I had attempted to match the cosmetic options of Duke Statistical Science courses wherever I could.\nAnother issue revealed in TA interviews was the question order. We originally arranged items near others testing a similar topic to facilitate our iterative editing. However, there was a clear case of cognitive priming in TA A’s interview. Having just correctly answered Image Recognition (ethics; representativeness of data), he then immediately jumped to examining ethical implications in the following item, Application Screening (ethics; proxy variable), while the faculty members who had answered the questions in a different order required more consideration here. As well, TA B was the first interviewee to answer incorrectly to the item I think is the single trickiest (#3 in He Said She Said (interpret visualization)), and best written in terms of getting students to think critically about what exactly plots are displaying. This stood out to me, as she was the only undergraduate interviewed; we observed a stratification of data science comfortability even among TAs. Finally, TA C was the quickest interviewee to run through the questions, answering all correctly and citing the logic we were looking for. As the most relative newcomer to the Duke Statistical Science program and having graduated from an undergraduate data science program that used Python, we interpreted his notable ease in getting through all questions in much less than an hour as a good sign that the current length might be ideal for students, who would be less experienced with the material but would also not have to think aloud or give feedback.\n\nRegrouping to synthesize feedback/final pilot assessment\nNot many changes were made following the student interviews in Phase B–a sign that we were converging on a viable prototype for distribution. To remedy the priming issue encountered in TA A’s interview, we shuffled the question order between the TA B and C interviews and landed on a solid layout: distributing topics well throughout and gradually ramping up our intended difficulty to prevent less-knowledgeable students from getting frustrated and giving up early on. As part of this pacing reform, we improved the flow of the assessment by reformatting Movie Budgets 2 (\\(R^2\\); compare trends visually) and Disease Screening (compare classification diagnostics visually) from groups of several nearly-identical items into single matrices.\n\n\nFinal assessment themes\nThe following table summarizes the learning objectives for each item in the most current version of the assessment.\n\n\n\n\n\n\n\nPassage\nLearning Objective(s)\n\n\n\n\nStorm Paths\nmodeling; simulated data; interpret uncertainty\n\n\nMovie Budgets 1\ncompare summary statistics visually\n\n\nMovie Budgets 2\nmodeling; \\(R^2\\); compare trends visually\n\n\nApplication Screening\nethics; modeling; proxy variable\n\n\nBanana Conclusions\ncausation; statistical communication\n\n\nCOVID Map\ninterpret complex visualization; spatial data; time series data\n\n\n\ninterpret complex visualization; sophisticated scales\n\n\nHe Said She Said\ninterpret basic visualization\n\n\n\ninterpret basic visualization\n\n\n\ninterpret basic visualization; sophisticated scales\n\n\nBuild-a-Plot\ndata to visualization process\n\n\nDisease Screening\ncompare classification diagnostics visually\n\n\nRealty Tree\nmodeling; regression tree\n\n\n\nmodeling; regression tree; variable selection\n\n\nWebsite Testing\ninterpret trends visually; visualize uncertainty; modeling; time series data\n\n\n\ninterpret trends visually; visualize uncertainty; modeling; time series data; extrapolation\n\n\n\ninterpret trends visually; visualize uncertainty; modeling; time series data; extrapolation\n\n\nImage Recognition\nethics; modeling; representativeness of training data\n\n\nData Confidentiality\nethics; data deidentification; statistical communication\n\n\nActivity Journal\nstructure data; store data\n\n\nMovie Wrangling\ndata cleaning; column-wise operations; string operations\n\n\n\ndata cleaning; column-wise operations; string operations\n\n\n\ndata cleaning; extrapolation\n\n\n\ndata wrangling; pseudocode; joins\n\n\n\ndata wrangling; pseudocode; joins\n\n\n\ndata wrangling; pseudocode; joins"
  },
  {
    "objectID": "development.html#phase-3-large-scale-student-pilot",
    "href": "development.html#phase-3-large-scale-student-pilot",
    "title": "Assessment Development",
    "section": "Phase 3: Large-scale Student Pilot",
    "text": "Phase 3: Large-scale Student Pilot\nLooking ahead, the next step after individual item tweaking and refinement will be real-world measurements of pacing and length and the feasibility for introductory data science students. Collaborating with the professors of Duke’s Introductory Data Science Course STA199, students across three sections will be invited to take the assessment as part of a typical homework assignment for the course. We will not solicit general feedback like in the previous interviews, instead asking just a single question at the end to get a rough estimate of how long it took to complete, since students will be completing it on their own time. The score students receive will not reflect their performance on the assessment, but rather will be simply awarded proportionally to how much was completed. To ensure students are actively engaging with the questions, there will be two attention checks added that must also be passed to earn credit.\nIn order to distribute the assessment at large and record students’ responses, the Quarto book has been converted into a Qualtrics survey. Once complete, large-scale analysis will be conducted, both to ensure that all questions are indeed appropriate for introductory level students and to explore potential instrument subscales.\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "package.html#phase-1-initial-tidying-work",
    "href": "package.html#phase-1-initial-tidying-work",
    "title": "Package Development",
    "section": "Phase 1: Initial tidying work",
    "text": "Phase 1: Initial tidying work\nDuring my first semester (Spring 2022) working on the project, our goal was to get myself oriented with learnr and the process of package creation on GitHub, and create all content for publication. At this time, the package had nine tutorials already created as well as their associated data sets and documentation.\nDuring this time, my first edits and associated pull requests mostly had to do with the wording of answer choices. Given the introductory nature of the tutorials, I added more descriptive language for explaining statistical concepts (e.g. “multiple peaks” vs “bimodal”) in several places. When giving feedback on a multiple choice question, gradethis allows you to specify a specific feedback message alongside each possible answer choice. In several places, I took advantage of these messages to give more scaffolded feedback on what was incorrect about the chosen answer, versus just “try again.”\nThe most substantial change I made was in the second tutorial, which explores UK traffic accident data. One question asked learners to filter for a particular level of the binary urban/rural variable and report how many rows remained. The data dictionary did not explain which level (“1” or “2”) corresponded to urban or rural, so I went searching in the data’s source link. The link, which was broken and I ended up replacing, gave me the necessary information. However, the levels were reversed from what was recorded as the correct answer in gradethis, and thus I had to edit the grading logic to reflect this change.\nWhile updating the data dictionary, I became well acquainted with the process for writing data documentation for R package publication. This involved creating a document following a specific LaTeX-style format, filling in the necessary background, variable descriptions, and example code, and letting the roxygen2 package generate the files. The result is the familiar, RStudio “Help” tab-style markdown documentation that exists for every released R function and dataset."
  },
  {
    "objectID": "package.html#phase-2-creating-a-new-tutorial",
    "href": "package.html#phase-2-creating-a-new-tutorial",
    "title": "Package Development",
    "section": "Phase 2: Creating a new tutorial",
    "text": "Phase 2: Creating a new tutorial\nOnce familiar with the package goals, structure, and build process, it was time to complete the scope of the tutorials by adding more. With nine tutorials already, the only homework assignment from the Data Science in a Box curriculum that had yet to be converted dealt with data on locations of Denny’s restaurants and LaQuinta inns. Skeleton .Rd files for these datasets already existed in the dsbox files, and thus I imagined the original plan for dsbox was to have a tenth tutorial working with these data.\nWhile the assignment existed in a typical format on the Data Science in a Box website, I was tasked with thinking critically about how to best translate the learning objectives into a learnr tutorial. Essentially, the key was mirroring the pacing between background information and coding exercises early on, mimicking the missing lecture component in these self-guided online tutorials. Breaking up exercises from single questions on the homework to a series of code steps was essential for flow, as well as to ensure that the final tasks would be possible even if the intermediate steps had not been entirely mastered. I also intentionally left the final set of exercises less scaffolded and somewhat open-ended, with the goal of serving as a more summative assessment of students’ data science skills over the course of all ten tutorials."
  },
  {
    "objectID": "package.html#phase-3-cran-submission",
    "href": "package.html#phase-3-cran-submission",
    "title": "Package Development",
    "section": "Phase 3: CRAN submission",
    "text": "Phase 3: CRAN submission\nIn the final semester of the project, the goal was to complete all necessary checks for publication to the Comprehensive R Archive Network (CRAN). CRAN, among other functions, hosts an archive of all packages published for the R programming language and facilitates an easy installation process, due to its rigorous specifications required when developres upload packages. However, we encountered a major roadblock when trying to release dsbox this spring 2023. One of the packages that dsbox depends on, gradethis, has not yet been released on CRAN itself. This prevents us from releasing a package that would require its installation.\nMy learning this semester mostly had to do with higher-level software development such as checks and automated GitHub Actions. In order to consider a package ready for publication, it must pass a series of CMD checks, which essentially ensure the source files can be properly installed and built on a variety of machines. The running of these checks is facilitated by GitHub Actions, which allows us to automatically run CMD checks every time we push new commits to GitHub.\nThe first task turned out to be updating the versions of these Actions checks, as it had been so long since they were run that the version of the checks used, version one, had been deprecated. This took some trial and error, but I eventually figured out how to replace the necessary pieces of each Action check while keeping the necessary dsbox-specific components. Inspired to update other deprecated elements, I replaced all instances of the magrittr pipe (%&gt;%) with the dependency-free base R pipe (|&gt;). With all tutorials using the base pipe, our package then necessitated a requirement of R version 4.1 or above. Other project-wide edits made at this stage were standardizing all tutorials to use American English, updating all broken links, and decreasing the sizes of tutorials’ cover photos to meet the 5MB maximum allowed for a package on CRAN.\nWhile updating the R version requirement, we took the time to ensure the rest of the DESCRIPTION file contained the necessary elements. When R packages are created, the necessary DESCRIPTION file contains information on basics like the package’s title, description, and version number, as well as details on which packages it depends on to be installed alongside it. Given our use of particular features in the gradethis package, we needed not only for it to be installed but also for it to be at least a specific version number. However, here we encountered the major roadblock that gradethis has not yet been released on CRAN itself.\nWe have submitted a feature request to the package developers to release a CRAN version–according to the website, the current version is passing all necessary CMD checks and has a healthy level of interest and engagement from the community. In the case that this does not occur in the near future, we hope to still publicize dsbox to the data science education community.\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "Discussion",
    "section": "",
    "text": "While my work on the assessment ended before a large-scale student pilot, we anticipate that the research team will be able to obtain a set of introductory data science student responses by the end of the Spring 2023 semester. As we work to draft a research proposal with Duke’s IRB, we hope to again evaluate the wording and pacing of the assessment on the target population, making additional refinements before a true pilot intended to actually measure students’ learning and explore its validity and possible subscales. As the team prepares for such a step, the focus has shifted towards obtaining funding from an NSF grant, possibly by either expanding the scope of the curriculum to K-12 instructors, or by drilling down technically and building the assessment into a more robust format via Javascript and other advanced tools.\nAlthough the assessment itself deals with introductory topics, I developed my knowledge of many advanced concepts in data science and programming through my work on this project. Summarizing into two general themes, working on the assessment and package has given me the unique opportunities to dive deeper into computer science topics such as web development and advanced GitHub usage, as well as to interact with others’ real-life code beyond the scope of the classroom or research lab. The first outcome is particularly relevant to me as I was not able to take as many computer science or statistical programming courses as I hoped at Duke. Thus, developing the GitHub Pages site with code styling, alt text, and navigation bars provided me with HTML, CSS, and Gith=Hub Actions experience that I would not have gotten otherwise. As well, my only previous exposure to GitHub had been on a smaller scale, such as class projects or smaller research teams. Working on a community-wide package like dsbox was my first time using GitHub’s more advanced features, such as forking branches of others’ functioning code, creating pull requests, and approving others’. It was also rewarding to learn about all that goes into creating an R package, from the file structure, .Rd files, to CMD checks. The process of iteratively attempting and failing to submit dsbox gave me a much deeper appreciation for the wide library of open source software we use daily as statisticians.\n\nAcknowledgements\nThis work would not have been possible without the incredible support and mentorship from my advisor, Dr. Mine Çetinkaya-Rundel, who has been by my side since the beginning of my Duke Statistical Science journey. I would also like to thank Drs. Alexander Fisher and Amy Herring for forming my committee and helping me make this work the best it can be. I have to acknowledge Dr. Joan Combs Durso, not just for her help with the logistical aspects of the Graduation with Distinction process, but also for her insightful workshops as well as everything she does to support the undergraduate student experience in the department. Of course, I also am immensely grateful for the three faculty members and students who offered their time to help improve our assessment. Special thanks goes again to Dr. Fisher, as well as Dr. Elijah Meyer, for graciously allowing us to use their STA199 students as subjects for our large-scale release, and specifically for working so well in tandem with us as we iterated the study design to satisfy IRB. Finally, I would be remiss not to thank my friends, family, and Statistical Science peers for providing feedback and emotional support, as well as every student for whom I have had the pleasure to be a TA, tutor, or study group facilitator, for helping me develop teaching and mentoring skills and for continuously reigniting my passion for both.\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Çetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look\nat Introductory Data Science.” Journal of Statistics and Data\nScience Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer,\nAndrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017.\n“Curriculum Guidelines for Undergraduate Programs in Data\nScience.” Annual Review of Statistics and Its\nApplication 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007.\n“Assessing Students’ Conceptual Understanding After a\nFirst Course in Statistics.” Statistics Education Research\nJournal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept\nInventoryMeasurement of the Effect of Teaching Methodology\nin Mathematics.” Notices of the American Mathematical\nSociety 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College\nCourse Placement Decisions Based on CLEP Exam Scores: CLEP Placement\nValidity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V.\nDiBello, and James W. Pellegrino. 2015. “An Analytic Framework for\nEvaluating the Validity of Concept Inventory Claims.” Journal\nof Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015.\n“2015 ASEE Annual Conference and Exposition.” In,\n26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory\nfor Alternate Conceptions Among First-Semester General Chemistry\nStudents.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela\nMeyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca\nNugent. 2022. “Think-Aloud Interviews: A Tool for Exploring\nStudent Statistical Reasoning.” Journal of Statistics and\nData Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe\nGibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram\nKrishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical\nSymposium on Computer Science Education.” In, 22–28. Providence\nRI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel,\nRegina F. Frey, and Paul S. Mattson. 2021. “Development and\nValidation of an Introductory Psychology Knowledge Inventory.”\nScholarship of Teaching and Learning in Psychology 7 (2):\n123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi\nSteinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE\nAnnual Conference & Exposition.” In, 30231. Salt Lake City,\nUtah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and\nUniversities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An\nOperational Definition Based on Text Mining of Data Science\nCurricula.” Journal of Behavioral Data Science 1 (1):\n1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "instrument.html#sec-storm-paths",
    "href": "instrument.html#sec-storm-paths",
    "title": "Appendix A: Assessment Prototype",
    "section": "Storm Paths",
    "text": "Storm Paths\nThe figure below shows a forecast after simulating 50 potential paths for a large storm. The two points (a) and (b) represent two cities. Which city is more likely to be hit by the storm? Explain.\n\n\n\n\n\n\n\n\n\n\nCity a\nCity b"
  },
  {
    "objectID": "instrument.html#sec-movie-budgets-1",
    "href": "instrument.html#sec-movie-budgets-1",
    "title": "Appendix A: Assessment Prototype",
    "section": "Movie Budgets 1",
    "text": "Movie Budgets 1\nA data scientist at IMDb has been given a dataset comprised of the revenues and budgets for 2,349 movies made between 1986 and 2016.\nSuppose they want to compare several distributional features of the budgets among four different genres—Horror, Drama, Action, and Animation. To do this, they create the following plots.\n\n\n\n\n\n\n\n\n\nFill in the following table by placing a check mark in the cells corresponding to the attributes of the data that can be determined by examining each of the plots.\n\n\n\n\n\n\n\n\n\n\n\nPlot A\nPlot B\nPlot C\nPlot D\n\n\n\n\nMean\n☐\n☐\n☐\n☐\n\n\nMedian\n☐\n☐\n☐\n☐\n\n\nIQR\n☐\n☐\n☐\n☐\n\n\nShape\n☐\n☐\n☐\n☐"
  },
  {
    "objectID": "instrument.html#sec-movie-budgets-2",
    "href": "instrument.html#sec-movie-budgets-2",
    "title": "Appendix A: Assessment Prototype",
    "section": "Movie Budgets 2",
    "text": "Movie Budgets 2\nFor each genre, the data scientist also fitted a regression line to model the relationship between movies’ budgets and their revenues. A scatterplot of this relationship, along with the fitted regression line, is shown for each of the four genres below. For which genre would the fitted regression model produce the highest \\(R^2\\) value? Explain."
  },
  {
    "objectID": "instrument.html#sec-application-screening",
    "href": "instrument.html#sec-application-screening",
    "title": "Appendix A: Assessment Prototype",
    "section": "Application Screening",
    "text": "Application Screening\nYou are working on a team that is making a deterministic model to quickly screen through applications for a new position at the company. Based on employment laws, your model may not include variables such as age, race, and gender, which could be potentially discriminatory.\nYour colleague suggests including a rule that eliminates candidates with more than 20 years of previous work experience, because they may have high salary expectations. Are there ethical implications of using this variable to select candidates? Explain your answer."
  },
  {
    "objectID": "instrument.html#sec-banana-conclusions",
    "href": "instrument.html#sec-banana-conclusions",
    "title": "Appendix A: Assessment Prototype",
    "section": "Banana Conclusions",
    "text": "Banana Conclusions\nData scientists at FiveThirtyEight administered a food frequency questionnaire. With 54 complete responses they found that people who ate bananas tended to score higher on the SAT verbal section than the SAT math section (\\(p=0.0073\\)). An article reporting the results of this study has the headline, “Eat more bananas to score higher on the SAT verbal section”. Is this headline accurate, or could it be misleading? Explain."
  },
  {
    "objectID": "instrument.html#sec-COVID-map",
    "href": "instrument.html#sec-COVID-map",
    "title": "Appendix A: Assessment Prototype",
    "section": "COVID Map",
    "text": "COVID Map\nThe visualization below displays the 14-day rolling average of new COVID-19 cases January 1 - August 31, 2021 in the United States. Each plot represents a state or Washington, D.C., and is labeled using the state’s abbreviation (e.g., MA = Massachusetts). The shaded area under each curve represents the increase in new cases since the state’s minimum point in 2021. This is a recreation of a similar plot that originally appeared in the New York Times.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we learn from this plot about COVID-19 cases in the US?\nCompare KY (Kentucky) in the South region to CA (California) in the West region. Based on this plot, can we conclude there was a difference in overall number of COVID cases in KY and CA in August 2021? Explain."
  },
  {
    "objectID": "instrument.html#sec-he-said-she-said",
    "href": "instrument.html#sec-he-said-she-said",
    "title": "Appendix A: Assessment Prototype",
    "section": "He Said She Said",
    "text": "He Said She Said\nFor each of the following items, indicate whether the statement is TRUE, FALSE, or whether you would need additional information to determine this. If you can determine the statement is true/false, indicate the evidence that you used to make that determination. If you need additional information to make that determination, indicate what else you would need.\n\n\n\n\n\n\n\n\n\nMen in Austen’s novels are more likely to have ‘dared’, ‘expected’, and ‘ran’ than women.\n\n\nTrue\nFalse\nNeed additional information to determine this\n\n\nWomen in Austen’s novels are more likely to have ‘remembered’, ‘felt’, and ‘cried’ than men.\n\n\nTrue\nFalse\nNeed additional information to determine this\n\n\nWomen in Austen’s novels are more likely to have ‘remembered’ than ‘feared’.\n\n\nTrue\nFalse\nNeed additional information to determine this"
  },
  {
    "objectID": "instrument.html#sec-build-a-plot",
    "href": "instrument.html#sec-build-a-plot",
    "title": "Appendix A: Assessment Prototype",
    "section": "Build-a-Plot",
    "text": "Build-a-Plot\nThe following is an intensity map of the unemployment rate among adults in the counties in the United States (based on data from 2019).\n\n\n\n\n\n\n\n\n\nIndicate which of the following data you need to recreate this map? (Select all that apply.)\n☐ County boundaries\n☐ Unemployment rate in each county\n☐ Number of adults living in each county\n☐ Number of unemployed adults living in each county\n☐ Total population of the county"
  },
  {
    "objectID": "instrument.html#sec-disease-screening",
    "href": "instrument.html#sec-disease-screening",
    "title": "Appendix A: Assessment Prototype",
    "section": "Disease Screening",
    "text": "Disease Screening\nCOVID screening tests are not 100% accurate. It’s possible to have COVID but not test positive or not have COVID but test positive for it. The following three visualizations display the outcomes of a COVID screening test with a sensitivity (true positive rate) of 98.1% and specificity (true negative rate) of 99.6% in a population where 5% of the individuals have COVID.\nWe are also interested in the false positive (individuals classified as with COVID, who don’t actually have it) and false negative (individuals classified as without COVID, but who do actually have it) rates.\n\n\n\n\n\n\n\n\n\nFill in the following table by placing a check mark in the cells corresponding to the attributes of the data that can be determined by examining each of the plots.\n\n\n\n\n\n\n\n\n\n\nPlot A\nPlot B\nPlot C\n\n\n\n\nSensitivity\n☐\n☐\n☐\n\n\nSpecificity\n☐\n☐\n☐\n\n\nFalse positive rate\n☐\n☐\n☐\n\n\nFalse negative rate\n☐\n☐\n☐"
  },
  {
    "objectID": "instrument.html#sec-realty-tree",
    "href": "instrument.html#sec-realty-tree",
    "title": "Appendix A: Assessment Prototype",
    "section": "Realty Tree",
    "text": "Realty Tree\nA realtor has trained a regression tree to predict the price of a house from features such as number of bedrooms, number of bathrooms, number of fireplaces, and size of the living area.\n\n\n\n\n\n\n\nTree\n\n  \n\na1\n\n Living Area   p &lt; .001   \n\nb1\n\n Living Area   p &lt; .001   \n\na1-&gt;b1\n\n    ≤ 1988 ft.²        \n\nb2\n\n Living Area   p &lt; .001   \n\na1-&gt;b2\n\n    &gt; 1988 ft.²   \n\nc1\n\n Living Area   p &lt; .001   \n\nb1-&gt;c1\n\n  ≤ 1483 ft.²                  \n\nc2\n\n Bathrooms   p &lt; .001   \n\nb1-&gt;c2\n\n     &gt; 1483 ft.²   \n\nc3\n\n Bathrooms   p &lt; .001   \n\nb2-&gt;c3\n\n    ≤ 2816 ft.²   \n\nc4\n\n Fireplaces   p &lt; .001   \n\nb2-&gt;c4\n\n    &gt; 2816 ft.²   \n\nd1\n\n Bathrooms   p &lt; .001   \n\nc1-&gt;d1\n\n     &gt; 1080 ft.²         \n\ne1\n\n n = 224   y = $130,444   \n\nc1-&gt;e1\n\n       ≤ 1080 ft.²          \n\ne4\n\n n = 229   y = $210,950   \n\nc2-&gt;e4\n\n   ≤ 1.5   \n\ne5\n\n n = 47   y = $258,345   \n\nc2-&gt;e5\n\n    &gt; 1.5   \n\nd2\n\n Living Area   p &lt; .001   \n\nc3-&gt;d2\n\n    &gt; 1.5   \n\ne8\n\n n = 209   y = $262,972   \n\nc3-&gt;e8\n\n    ≤ 5   \n\ne9\n\n n = 82   y = $326,267   \n\nc4-&gt;e9\n\n     ≤ 1   \n\ne10\n\n n = 15   y = $501,876   \n\nc4-&gt;e10\n\n     &gt;1   \n\ne2\n\n n = 336   y = $151,424   \n\nd1-&gt;e2\n\n   ≤ 1.5        \n\ne3\n\n n = 238   y = $184,248   \n\nd1-&gt;e3\n\n    &gt; 1.5   \n\ne6\n\n n = 45   y = $214,802   \n\nd2-&gt;e6\n\n     ≤ 2576 ft.²             \n\ne7\n\n n = 102   y = $294,349   \n\nd2-&gt;e7\n\n    &gt; 2576 ft.²  \n\n\n\n\n\nWhat price would the tree predict for a house with 3200 ft.2 of living area, 1.5 bathrooms, and 1 fireplace?\n\n$262,972\n$326,267\n$501,876\nCan’t be determined from the information given.\n\nWhat price would the tree predict for a house with 1200 ft.2 of living area and 5 bathrooms?\n\n$151,424\n$184,248\n$210,950\nCan’t be determined from the information given."
  },
  {
    "objectID": "instrument.html#sec-website-testing",
    "href": "instrument.html#sec-website-testing",
    "title": "Appendix A: Assessment Prototype",
    "section": "Website Testing",
    "text": "Website Testing\nAn e-commerce company is working on their website design and is interested in knowing whether having the website mainly in blue or red would lead to better business outcomes. One outcome they are measuring is the number of returning users to the website. They design two versions of the website one in blue and the other in red. A random half of the visitors see the website in blue and the other half see it in red. The plot shows the number of returning users per day for the two different versions of the website.\n\n\n\n\n\n\n\n\n\nIndicate whether each of the following conclusions are valid. Explain.\nOver time the company is getting more returning users regardless of the version of the website.\n\nValid\nInvalid\nCannot determine this from the plot.\n\nOn the 31st day, the blue version of the website is expected to have higher number of returning users.\n\nValid\nInvalid\nCannot determine this from the plot.\n\nOn the 60th day, the blue version of the website is expected to have higher number of returning users.\n\nValid\nInvalid\nCannot determine this from the plot."
  },
  {
    "objectID": "instrument.html#sec-image-recognition",
    "href": "instrument.html#sec-image-recognition",
    "title": "Appendix A: Assessment Prototype",
    "section": "Image Recognition",
    "text": "Image Recognition\nA data science student wants to create an image recognition algorithm to identify whether a university professor belongs to a department in the sciences or not. To do this, she collects data by scraping several university photo archives of university faculty. She labels faculty in the photos as “Sciences” or “Not sciences”. The images below depict a small representative sample of her data.\nSciences\n\n\n\n\n\n\n\n\n\nNot sciences\n\n\n\n\n\n\n\n\n\nThe data science student plans to use photos of current university faculty to predict whether they are scientists. What concerns might you have about the predictions from this algorithm? Explain."
  },
  {
    "objectID": "instrument.html#sec-data-confidentiality",
    "href": "instrument.html#sec-data-confidentiality",
    "title": "Appendix A: Assessment Prototype",
    "section": "Data Confidentiality",
    "text": "Data Confidentiality\nA newspaper reports on the results of a survey from a small (&lt;2000 student) university. The university agrees to have the data released to the public so long as the students’ identities and academic standing information are kept confidential. Select the safe combinations of variables that are unlikely to identify any individual students. Explain.\n\n\nClass year and sports played\nStudent ID and dorm ZIP code\nGPA and major\nBirth date and phone number\nNone of the above"
  },
  {
    "objectID": "instrument.html#sec-activity-journal",
    "href": "instrument.html#sec-activity-journal",
    "title": "Appendix A: Assessment Prototype",
    "section": "Activity Journal",
    "text": "Activity Journal\nBelow is data that was recorded in an activity journal.\n\n\n\n\n\n\n\n\n\nA data scientist reformats the data into a table so that each variable represented in the data is recorded in a single column. Describe what each of the columns of this table will contain, as well as what each row or observation of the table will represent."
  },
  {
    "objectID": "instrument.html#sec-movie-wrangling",
    "href": "instrument.html#sec-movie-wrangling",
    "title": "Appendix A: Assessment Prototype",
    "section": "Movie Wrangling",
    "text": "Movie Wrangling\nThe table below provides data about 10 movies released in the United States. It provides data on the movie’s title (title), the movie’s director (director), the date the movie was released (release_date), the season the movie was released (season), the worldwide gross intake in U.S. dollars (gross), the cleaned version of the worldwide gross intake in U.S. dollars (gross_clean), and whether or not the movie won the Best Picture Oscar (best_picture).\n\n\n\nMovies Table\n\n\ntitle\ndirector\nrelease_date\nseason\ngross\ngross_clean\nbest_picture\n\n\n\n\nAlmost Famous\nCameron Crowe\n22 September 2000\nFall\n$47.39M\n47.39\nNo\n\n\nCODA\nSian Heder\n13 August 2021\nSummer\n$1.61M\n1.61\nYes\n\n\nE.T. the Extra-Terrestrial\nSteven Spielberg\n11 June 1982\nSummer\n$792.91M\n792.91\nNo\n\n\nLuca\nEnrico Casarosa\n18 June 2021\nSummer\n$49.75M\n49.75\nNo\n\n\nMiddle of Nowhere\nAva DuVernay\n1 September 2014\nFall\n$0.24M\n0.24\nNo\n\n\nMoonlight\nBarry Jenkins\n18 November 2016\nFall\n$65.34M\n65.34\nYes\n\n\nParasite\nBong Joon Ho\n8 November 2019\nFall\n$262.69M\n262.69\nYes\n\n\nSay Anything\nCameron Crowe\n14 April 1989\nSpring\n$21.52M\n21.52\nNo\n\n\nSelma\nAva DuVernay\n9 January 2015\nWinter\n$66.79M\n66.79\nNo\n\n\nWe Bought a Zoo\nCameron Crowe\n23 December 2011\nWinter\n$120.08M\n120.08\nNo\n\n\n\n\n\n\n\nDescribe a process that you could use to generate the data in the season column using the information in the release_date column.\nDescribe a process that you could use to generate the data in the gross_clean column using the information in the gross column.\nYou have been tasked with adding a new column called nominated_for_best_picture which indicates whether or not each movie was nominated for the Best Picture Oscar (“Yes” if it was, “No” if it was not). Is there sufficient information in this dataset to generate this new column? Explain.\nThe table below provides data about 10 movie directors. It provides data on the director’s name (director), the number of Oscars the movie’s director has been nominated for (nominations), and the number of Oscars the director has won (oscars).\n\n\n\nDirectors Table\n\n\ndirector\nnominations\noscars\n\n\n\n\nAva DuVernay\n1\n0\n\n\nBarry Jenkins\n3\n1\n\n\nBong Joon Ho\n3\n3\n\n\nCameron Crowe\n3\n1\n\n\nEnrico Casarosa\n2\n0\n\n\nLoveleen Tandan\n0\n0\n\n\nNora Ephron\n3\n0\n\n\nPenny Marshall\n0\n0\n\n\nSian Heder\n1\n1\n\n\nSteven Spielberg\n19\n3\n\n\n\n\n\n\n\nUse the data in the Movies Table and in the Directors Table to answer the following questions. For each question, what is the result of carrying out the given pseudocode (ie. code recipe)?\n1.\n\nSTART_WITH(the Movies table) then\n    KEEP_ROWS_WHERE(the season value is Fall) then\n    COUNT(the number of rows)\n\n2.\n\nSTART_WITH(the Movies table) then\n    KEEP_ROWS_WHERE(the season value is Fall) then\n    COUNT(the number of rows) WHERE( best_picture value is Yes)\n\n\n\n\n\n\n\n\n\n\n3.\n\nSTART_WITH(the Movies table) then\n    KEEP_ROWS_WHERE(the season value is Fall then\n    ADD_COLUMNS_FROM(the Director Table) MATCHING_BY(the director column) then\n    COUNT(the number of rows) WHERE(oscars value is 3) AND(best_picture value is No)\n\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "graveyard.html#sec-bikes-scooters-1",
    "href": "graveyard.html#sec-bikes-scooters-1",
    "title": "Appendix B: Item Graveyard",
    "section": "Bikes and Scooters 1",
    "text": "Bikes and Scooters 1\nAs a way to help the environment some cities in the U.S. are adding bike and e-scooter share stations which allow people to rent a bike or e-scooter for commuting or pleasure. The bikes and scooters are often kept at electronic docking stations at multiple locations around the cities. The following graphs were created using data from the Department of Transportation Statistics1 about public use of these shared dock systems in four U.S. cities—Chicago, Minneapolis, Portland, and Topeka.\nWhich of the four cities had the most bike docks in 2020? Explain how you determined this. Or if you cannot answer it from the visualization, explain why not.\n\n\n\n\n\n\n\n\n\n\nThe bar chart (a) shows the the percentage of docks in each city that were bike docks for each year from 2015–2020.\nThe pie chart (b) was created by plotting the percentage of bike docks for each year in one particular city. Unfortunately, the data scientist has forgotten which city this is. Using the information in the bar chart, identify the city. Explain how you determined this or if you cannot answer it from the visualization, explain why not.\n\n\n\n\n\n\n\n\n\nWhich of the four cities had the biggest increase in the number of bike docks from 2018 to 2020? Explain how you determined this or if you cannot answer it from the following line graph visualization, explain why not."
  },
  {
    "objectID": "graveyard.html#sec-bikes-scooters-2",
    "href": "graveyard.html#sec-bikes-scooters-2",
    "title": "Appendix B: Item Graveyard",
    "section": "Bikes and Scooters 2",
    "text": "Bikes and Scooters 2\nThe map below shows the number of cities in each region of the United States that have docked bikes, dockless bikes, or e-Scooters in both 2018 and 2020. Use that information to answer each of the following questions. For each question, explain how you determined your answer, or if you cannot answer it from the visualization, explain why not.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n2020\n\n\nDocked Bikes\nDockless Bikes\ne-Scooters\nDocked Bikes\nDockless Bikes\ne-Scooters\n\n\n\n\nWest\n17\n14\n7\n13\n7\n3\n\n\nSouthwest\n11\n7\n5\n8\n6\n4\n\n\nMidwest\n27\n5\n8\n16\n6\n6\n\n\nNortheast\n17\n18\n5\n10\n3\n1\n\n\nSoutheast\n29\n13\n9\n14\n11\n11\n\n\n\n\n\n\n\n\nHow many cities in the Southeast had e-Scooters in 2018?\nIn 2020, the Southwest region has more docked bike stations than dockless bike stations.\nThe majority of regions decreased the number of e-scooter stations from 2018 to 2020.\nAcross the majority of regions, the trend is that over time, cities tend to be adopting dockless bikes rather than docked bikes.\nAcross the majority of regions, the trend is that over time, there are fewer cities that are making docked bikes, dockless bikes, and e-scooters available. Explain how you determined this or if you cannot answer it from the visualization, explain why not."
  },
  {
    "objectID": "graveyard.html#sec-bikes-scooters-3",
    "href": "graveyard.html#sec-bikes-scooters-3",
    "title": "Appendix B: Item Graveyard",
    "section": "Bikes and Scooters 3",
    "text": "Bikes and Scooters 3\nAn electric bike, also known as an e-bike, is a bicycle with a battery-powered “assist” that comes via pedaling. An online product recommendation service that tests and reviews products has gathered a representative sample of 15 e-bikes from a single manufacturer and measured their ranges (how far they can go on a full battery without recharging). Based on this sample, they calculated an average range of 60 kilometers, plus or minus 10 kilometers. Suppose you’re in the market for an e-bike and during your research you come across the following two items:\n\nAn e-bike with a range of 85 kilometers.\nA report from a different product recommendation service that has also gathered data from a different, but also representative sample of 15 e-bikes from this same manufacturer, with a mean range of 85 kilometers.\n\nWhich one of these make you doubt the original report more?"
  },
  {
    "objectID": "graveyard.html#sec-births-per-day",
    "href": "graveyard.html#sec-births-per-day",
    "title": "Appendix B: Item Graveyard",
    "section": "Births per Day",
    "text": "Births per Day\nA data scientist for a large urban hospital examined a sample of data to estimate the mean number of births that took place on Fridays and Saturdays. The plots below show the number of births that took place on either a Friday or Saturday for that sample.\n\n\n\n\n\n\n\n\n\nTo estimate the mean number of births that took place on Fridays and Saturdays, the data scientist computed confidence intervals (mean \\(\\pm\\) margin of error) for both days. Unfortunately they forgot which mean and margin of error was associated with each day.\nWhich mean is associated with Friday? Explain.\n\n8350\n11,800\n\nWhich margin of error is associated with Friday? Explain.\n\n100\n280"
  },
  {
    "objectID": "graveyard.html#sec-movie-budgets-3",
    "href": "graveyard.html#sec-movie-budgets-3",
    "title": "Appendix B: Item Graveyard",
    "section": "Movie Budgets 3",
    "text": "Movie Budgets 3\nThe data scientist was asked to use the fitted regression model to make a prediction for the revenue for a horror movie using two different potential budgets; a budget of \\(\\$25\\mathrm{M}\\) and a budget of \\(\\$50\\mathrm{M}\\) They were also asked to compute a prediction interval for these two predictions to estimate the uncertainty in the prediction. The scatterplot and fitted regression line for Horror movies is displayed below.\n\n\n\n\n\n\n\n\n\nWhich of the predictions would have a greater predicted revenue associated with it? Explain\n\n\n\\(\\mathrm{budget} = \\$25\\mathrm{M}\\)\n\\(\\mathrm{budget} = \\$50\\mathrm{M}\\)\nThey are the same.\nNot enough information to determine this."
  },
  {
    "objectID": "graveyard.html#sec-model-comparison",
    "href": "graveyard.html#sec-model-comparison",
    "title": "Appendix B: Item Graveyard",
    "section": "Model Comparison",
    "text": "Model Comparison\nA data scientist has trained four different classification models (null model, Naive Bayes model, k Nearest Neighbors (kNN) model, and random forest model) on a set of data. The observed responses and the model predictions for a set of 10 observations from a validation set of data are shown in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction\n\n\n\nObserved Responses\nNull\nNaive Bayes\nkNN\nRandom Forest\n\n\n\n\nNo\nNo\nNo\nNo\nNo\n\n\nNo\nNo\nYes\nYes\nNo\n\n\nNo\nNo\nNo\nNo\nNo\n\n\nNo\nNo\nNo\nNo\nNo\n\n\nYes\nNo\nNo\nNo\nNo\n\n\nNo\nNo\nYes\nYes\nNo\n\n\nNo\nNo\nNo\nNo\nNo\n\n\nNo\nNo\nNo\nNo\nNo\n\n\nYes\nNo\nYes\nYes\nYes\n\n\nYes\nNo\nYes\nYes\nYes\n\n\n\n\n\n\n\nAre the predictions from the kNN model more, less, or equally as accurate as the results from the null model? Explain."
  },
  {
    "objectID": "graveyard.html#sec-training-or-validation",
    "href": "graveyard.html#sec-training-or-validation",
    "title": "Appendix B: Item Graveyard",
    "section": "Training or Validation",
    "text": "Training or Validation\nThe figure shows a plot of prediction error as a function of model complexity for a training and validation sample. Which sample (training or validation) is associated with the orange, solid line? Explain.\n\n\n\n\n\n\n\n\n\n\nThe following three items were reworked into one context, as the current Movie Wrangling (pseudocode; column-wise operations; joins)."
  },
  {
    "objectID": "graveyard.html#sec-tv-show-wrangling",
    "href": "graveyard.html#sec-tv-show-wrangling",
    "title": "Appendix B: Item Graveyard",
    "section": "TV Show Wrangling",
    "text": "TV Show Wrangling\nThe two tables below provide data about several TV shows.\n\n\n\n\n\nCreator Table\n\n\nCreator\nTV_Show\nTV_Show_ID\n\n\n\n\nAguirre-Sacasa, Roberto\nRiverdale\nI\n\n\nBlair, April\nAll-American\nA\n\n\nDunham, Lena\nGirls\nL\n\n\nGlover, Donald\nAtlanta\nB\n\n\nLevitan, Steven\nModern Family\nF\n\n\nLloyd, Christopher\nModern Family\nF\n\n\nMurphy, Kevin\nHellcats\nE\n\n\nScheuring, Paul T.\nPrison Break\nH\n\n\nSherman-Palladino, Amy\nBunheads\nC\n\n\nSherman-Palladino, Amy\nGilmore Girls\nD\n\n\nStar, Darren\nSex and the City\nM\n\n\nStar, Darren\nYounger\nK\n\n\nWatson, Sarah\nThe Bold Type\nJ\n\n\n\n\n\n\n\n\nTV Show Table\n\n\nTV_Show\nTV_Show_ID\nNetwork\nSeasons\n\n\n\n\nAll-American\n01\nCW\n4\n\n\nAtlanta\n02\nFX\n4\n\n\nBunheads\n03\nABC Family\n2\n\n\nGilmore Girls\n04\nWB\n7\n\n\nHellcats\n05\nCW\n1\n\n\nModern Family\n06\nABC\n11\n\n\nOzark\n07\nNetflix\n4\n\n\nPrison Break\n08\nFox\n6\n\n\nRiverdale\n09\nCW\n6\n\n\nThe Bold Type\n10\nFreeform\n5\n\n\nYounger\n11\nTV Land\n7\n\n\n\n\n\n\n\n\n\n\n\n\nConsider the two following sets of pseudocode (ie. code recipe). Would they produce the same results? Explain.\n\nA.\n\nstart_with(the Creator table) and_then\n    add_columns_from(the TV Show table matching_by the TV Show column) and_then\n    count_of(the number of rows for the CW network)\n\nB.\n\nstart_with(the TV Show table) and_then\n    add_columns_from(the Creator table matching_by the TV Show column) and_then\n    count_of(the number of rows for the CW network)\n\n\n\n\n\n\n\n\nConsider the two following sets of pseudocode (ie. code recipe). Would they produce the same results? Explain.\n\nA.\n\nstart_with(the Creator table) and_then\n    add_columns_from(the TV Show table matching_by the TV Show column) and_then\n    count_of(the number of rows for the CW network)\n\nB.\n\nstart_with(the TV Show table) and_then\n    add_columns_from(the Creator table matching_by the TV Show ID column) and_then\n    count_of(the number of rows for the CW network)"
  },
  {
    "objectID": "graveyard.html#sec-shopping-wrangling",
    "href": "graveyard.html#sec-shopping-wrangling",
    "title": "Appendix B: Item Graveyard",
    "section": "Shopping Wrangling",
    "text": "Shopping Wrangling\nThe dataset below contains information on 8 people; we know their names and how many items they purchased online today.\n\n\n\n\n\nname\nnumber_of_items\nvisited_online_retailer\n\n\n\n\nMiriam\n10\n\n\n\nMarcel\n2\n\n\n\nAyesha\n0\n\n\n\nRebecca\n3\n\n\n\nLola\n0\n\n\n\nLaurence\n1\n\n\n\nTomos\n9\n\n\n\nAbdul\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou have been tasked with adding a new column called visited_online_retailer which indicates whether or not each person visited the website of an online retailer (“yes” if they did, “no” if they did not). Is there sufficient information in this dataset to generate this new column? Explain."
  },
  {
    "objectID": "graveyard.html#sec-park-wrangling",
    "href": "graveyard.html#sec-park-wrangling",
    "title": "Appendix B: Item Graveyard",
    "section": "Park Wrangling",
    "text": "Park Wrangling\nThe data set park_visits contains the number of annual visitors to 376 national park sites in the United States from 1904–2016. The data were originally collected from the National Park Service. There are 20,920 total records in the data set, since the parks were open for the entire date range. A few rows of park_visits data are shown below.\n\n\n\nyear\nstate\npark_site\nvisitors\n\n\n\n\n1904\nAR\nHot Springs National Park\n101000\n\n\n1904\nCA\nKings Canyon National Park\n1000\n\n\n1904\nOR\nCrater Lake National Park\n1500\n\n\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\n\n2016\nWY\nDevils Tower National Monument\n496210\n\n\n2016\nWY\nFort Laramie National Historic Site\n57444\n\n\n\nA data scientist would like to find the most popular park in each state in 2016. To do so, they decided to create a new data table named most_visited_2016 that includes the national park site in each state with the most visitors in 2016. The final table will include 51 rows (one for each state and Washington D.C.) and the columns year, state, park_site and visitors. Four of the 51 rows of the table are shown below.\n\n\n\n\n\n\n\n\n\nyear\nstate\npark_site\nvisitors\n\n\n\n\n2016\nAK\nKlondike Gold Rush National Historical Park\n912351\n\n\n2016\nAL\nLittle River Canyon National Preserve\n462700\n\n\n2016\nAR\nBuffalo National River\n1785359\n\n\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\n\n2016\nWY\nYellowstone National Park\n4257177\n\n\n\nArrange the steps to get from the original data set park_visits to the final table most_visited_2016. \n\n\n\nSteps:\n\nStart with park_visits\nFILTER(year == 2016): Filter for observations in 2016\nGROUP_BY(state): Group by state/Perform subsequent lines of code within each state\nARRANGE(DESC((visitors)): Sort the number of visits in descending order\nSLICE(1) : Take the first observation.\nEnd with most_visited_2016"
  },
  {
    "objectID": "graveyard.html#sec-data-cleaning",
    "href": "graveyard.html#sec-data-cleaning",
    "title": "Appendix B: Item Graveyard",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nA researcher randomly selects 10 students in a school and collects data about their age and number of siblings. They enter the data into a spreadsheet and are interested in calculating descriptive statistics. The software that they are using displays the data as shown below.\n\n\n\nrow\nX1\nX10\nX3\n\n\n\n\n1\n2\n11\n1\n\n\n2\n3\n12\n3\n\n\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\n\n8\n9\n12\n2\n\n\n9\n10\n8\n0\n\n\n\n\n\n\n\n\n10\nTotal\n113\n21\n\n\n\nWould it be safe to assume that the average age of children in the sample is 11.3? Explain.\nIf you could change the way that the data are displayed in this software, would you change anything? If yes, then list the thing(s) that you would change.\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  },
  {
    "objectID": "graveyard.html#footnotes",
    "href": "graveyard.html#footnotes",
    "title": "Appendix B: Item Graveyard",
    "section": "",
    "text": "Data from: https://data.bts.gov/d/7m5x-ubud/visualization↩︎"
  },
  {
    "objectID": "package-examples.html",
    "href": "package-examples.html",
    "title": "Appendix C: dsbox in action",
    "section": "",
    "text": "The following screenshots show the user experience with the tutorials from dsbox in the RStudio IDE.\n\n\n\n\n\nMain menu of the application\n\n\n\n\n\n\n\n\n\nTitle page of Tutorial 4: Lego Sales\n\n\n\n\n\n\n\n\n\nA sample question with associated code chunk\n\n\n\n\n\n\n\n\n\nA sample hint available while working in the code chunk\n\n\n\n\n\n\n\n\n\nSample “success” message upon getting the desired code output\n\n\n\n\n\n\n\n\n\nSample “incorrect” message to a radio button question\n\n\n\n\nUsers have the option to retry a missed question unlimited times.\n\n\n\n\n\nSample “correct” message to a radio button question\n\n\n\n\n\n\n\n\n\nCompletion screen\n\n\n\n\nUpon finishing a tutorial, the user is presented with an option to review specific sections, or to return to the main menu and select another tutorial.\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26. https://doi.org/10.1080/10691898.2020.1804497.\n\n\nDe Veaux, Richard D., Mahesh Agarwal, Maia Averett, Benjamin S. Baumer, Andrew Bray, Thomas C. Bressoud, Lance Bryant, et al. 2017. “Curriculum Guidelines for Undergraduate Programs in Data Science.” Annual Review of Statistics and Its Application 4 (1): 15–30. https://doi.org/10.1146/annurev-statistics-060116-053930.\n\n\nDelmas, Robert C., Joan Garfield, Ann Ooms, and Beth L. Chance. 2007. “Assessing Students’ Conceptual Understanding After a First Course in Statistics.” Statistics Education Research Journal 6: 28–58. https://doi.org/10.52041/serj.v6i2.483.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept InventoryMeasurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (08): 1018. https://doi.org/10.1090/noti1033.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. “Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report.” https://eric.ed.gov/?id=ED574772.\n\n\nJorion, Natalie, Brian D. Gane, Katie James, Lianne Schroeder, Louis V. DiBello, and James W. Pellegrino. 2015. “An Analytic Framework for Evaluating the Validity of Concept Inventory Claims.” Journal of Engineering Education 104 (4): 454–96. https://doi.org/10.1002/jee.20104.\n\n\nJorion, Natalie, Brian Gane, Louis DiBello, and James Pellegrino. 2015. “2015 ASEE Annual Conference and Exposition.” In, 26.497.1–12. Seattle, Washington: ASEE Conferences. https://doi.org/10.18260/p.23836.\n\n\nMulford, Douglas R., and William R. Robinson. 2002. “An Inventory for Alternate Conceptions Among First-Semester General Chemistry Students.” Journal of Chemical Education 79 (6): 739. https://doi.org/10.1021/ed079p739.\n\n\nReinhart, Alex, Ciaran Evans, Amanda Luby, Josue Orellana, Mikaela Meyer, Jerzy Wieczorek, Peter Elliott, Philipp Burckhardt, and Rebecca Nugent. 2022. “Think-Aloud Interviews: A Tool for Exploring Student Statistical Reasoning.” Journal of Statistics and Data Science Education 30 (2): 100–113. https://doi.org/10.1080/26939169.2022.2063209.\n\n\nSchanzer, Emmanuel, Nancy Pfenning, Flannery Denny, Sam Dooman, Joe Gibbs Politz, Benjamin S. Lerner, Kathi Fisler, and Shriram Krishnamurthi. 2022. “SIGCSE 2022: The 53rd ACM Technical Symposium on Computer Science Education.” In, 22–28. Providence RI USA: ACM. https://doi.org/10.1145/3478431.3499311.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7 (2): 123–39. https://doi.org/10.1037/stl0000172.\n\n\nStudy, Nancy, Steven Nozaki, Sheryl Sorby, Mary Sadowski, Heidi Steinhauer, Ronald Miller, and Kaloki Nabutola. 2018. “2018 ASEE Annual Conference & Exposition.” In, 30231. Salt Lake City, Utah: ASEE Conferences. https://doi.org/10.18260/1-2--30231.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” https://ryanswanstrom.com/colleges/.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16. https://doi.org/10.35566/jbds/v1n1/p1."
  }
]