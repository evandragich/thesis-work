[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Data Science Education: From Tutorials to Assessment",
    "section": "",
    "text": "Abstract\n\nPresentation Title: *\nCreating a standardized assessment to measure learning in introductory data science courses\n\n\nBrief Abstract:\nAs data science (DS) continues to grow in popularity among university course offerings, it is becoming crucial to successfully measure students’ learning outcomes in introductory courses. To do this requires an assessment designed to which could additionally be used to evaluate pedagogical techniques or curriculum interventions in data science curriculum\nTo develop a blueprint for the assessment, a multi-institutional team of statistics and data science education researchers identified common DS content (e.g., data wrangling, interpreting visualizations), drawing from published guidelines/recommendations and introductory DS syllabi. A draft of the assessment was written and used to conduct three think-aloud interviews with field-relevant faculty members.The interviews consisted of both open-ended brainstorming on the assessment’s scope as well as individual examinations of each item for relevance, clarity, and efficacy in measuring the desired learning objective. Think-aloud interviews were also conducted with introductory DS students to gauge item clarity and gain insight into the reasoning for their responses.\nThis poster includes the blueprint developed, as well as example items, and results from the faculty and student think aloud interviews. We also present next steps for the project including plans for larger scale piloting and further analyses.\n\n\nGoals\nBy sharing the work, we hope that participants will become familiar with an assessment they may use for designing intro data science curriculum or researching classroom innovations. We also hope that this instrument can serve as an inspiration or a starting point to be tailored by future researchers more specifically to their courses or to another discipline (eg. by adding more programming concepts instead of data visualization to better serve a computing-focused introductory data science class, etc.)\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26.\n\n\nDelmas, Robert, Joan Garfield, Ann Ooms, and Beth Chance. 2007. “ASSESSING STUDENTS’ CONCEPTUAL UNDERSTANDING AFTER A FIRST COURSE IN STATISTICS.” STATISTICS EDUCATION RESEARCH JOURNAL 6 (2): 28–58.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept Inventory-Measurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (8): 1018–27.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report. College Board.\n\n\nMulford, DouglasRobert. 1996. “An Inventory for Measuring College Students’ Level of Misconceptions in First Semester Chemistry.” Unpublished Master’s Thesis, Purdue University, IN.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7: 123–39.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” http://datascience.community/colleges.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16."
  },
  {
    "objectID": "01-thesis-intro.html",
    "href": "01-thesis-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "next section to write after development: just like about the motivation for the project\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26.\n\n\nDelmas, Robert, Joan Garfield, Ann Ooms, and Beth Chance. 2007. “ASSESSING STUDENTS’ CONCEPTUAL UNDERSTANDING AFTER A FIRST COURSE IN STATISTICS.” STATISTICS EDUCATION RESEARCH JOURNAL 6 (2): 28–58.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept Inventory-Measurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (8): 1018–27.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report. College Board.\n\n\nMulford, DouglasRobert. 1996. “An Inventory for Measuring College Students’ Level of Misconceptions in First Semester Chemistry.” Unpublished Master’s Thesis, Purdue University, IN.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7: 123–39.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” http://datascience.community/colleges.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16."
  },
  {
    "objectID": "02-background.html#background",
    "href": "02-background.html#background",
    "title": "Assessment Background",
    "section": "Background",
    "text": "Background\nAn essential component of any educational research is a validated, relevant instrument to measure students’ learning outcomes. Whether to award college credit like the College Board’s AP and CLEP exams, to measure students’ previously-held misconceptions, or just as a tool upon which to evaluate educational interventions, such assessments have been developed and statistically analyzed for a wide range of subjects such as Spanish, Psychology, Chemistry, and Calculus (Godfrey and Jagesic 2016; Solomon et al. 2021; Mulford 1996; Epstein 2013).\nIn the field of statistics, previous work on measuring students’ reasoning skills led to the development of the Comprehensive Assessment of Outcomes in Statistics (CAOS). The revised CAOS 4, comprising 40 multiple-choice items on a variety of commonly-taught first-semester introductory concepts, was first administered in 2005 and allowed instructors to measure whether their courses were successfully resulting in their desired learning outcomes. However, many instructors noted that their findings reflected a much lower understanding than expected, specifically regarding the topics of data visualization and data collection (Delmas et al. 2007). However, a key feature of the CAOS was the lack of hard computation nor need to recall specific formulas or definitions, allowing greater accessibility for a variety of statistics-adjacent uses.\nIn 2022, as more universities begin to support the emerging field of data science via specific courses, concentrations, or even majors, there is a need to measure students’ learning outcomes in these introductory classes analogously to the subjects named above (Swanstrom, n.d.). Lacking a clearly-defined scope, empirical studies of so called “data science” curricula suggest that the field can be thought of an augmentation of traditional statistical modeling concepts, with emphases on computing, data visualization and manipulation, as well as a consideration of ethics and the role data plays in society (Zhang and Zhang 2021).\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26.\n\n\nDelmas, Robert, Joan Garfield, Ann Ooms, and Beth Chance. 2007. “ASSESSING STUDENTS’ CONCEPTUAL UNDERSTANDING AFTER A FIRST COURSE IN STATISTICS.” STATISTICS EDUCATION RESEARCH JOURNAL 6 (2): 28–58.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept Inventory-Measurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (8): 1018–27.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report. College Board.\n\n\nMulford, DouglasRobert. 1996. “An Inventory for Measuring College Students’ Level of Misconceptions in First Semester Chemistry.” Unpublished Master’s Thesis, Purdue University, IN.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7: 123–39.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” http://datascience.community/colleges.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16."
  },
  {
    "objectID": "03-development.html",
    "href": "03-development.html",
    "title": "Assessment Development",
    "section": "",
    "text": "started with all questions in one document, joined with some discussions on items coverage and stuff put stuff in easy format to browse\nhighlight a few things: one thing that hadn’t really thought about that Kristin pointed out, something we hadnt thoguht about that Amelia/Nick pointed out. one paragraph each, nothing crazy but just explaining what its like to get feedback\nwith students with exposure: goal is to see if these topics are in your wheelhouse and if wording makes sense as a student\nbig themes: balancing size and pacing,\nsomethign good would be good to have content: use 15 passages and assign them to rough buckets\n\nPhase 0: Initial Cleaning and Feedback\nIn January 2022, I inherited a repo with several documents: many were background information on what topics would be included, with one containing all currently-written questions. These questions were not yet organized into specific groups of stem and items, many sections were commented out or overwritten, and it was clearly something that had been written piecewise by a group of people. My first true task was to run through the current questions myself, answer them how I would, and provide feedback on clarity, wording, and reflect on the topic covered.\nFrom this initial feedback, I created three pull requests attempting to solve some of these issues. These first fixes were minorly substantial edits: changing “most” to “the majority of” to make a question less ambiguous, adding a “fill-in-the-blank” slot to make even more clear what the question is asking, and background information on confidence intervals to an item that referenced them. This was my first exposure to what I would call “advanced” GitHub usage, or more than just typical cloning, pushing, and pulling from ordinary group projects. I used the very helpful pr_*() series of functions from the usethis package, which allowed me to easily create the pull requests for review.\nWhile the team discussed my pull request proposals, I worked in parallel to clean up any “obvious” fixes to all items. These “no-brainer” edits were mainly typos, distorted or obscured plots and items that don’t actually pose a question. It was also time to clean up the current document by converting it to a Quarto book, which would allow for easy webpage-like navigation. This marked my first exposure to Quarto, and I quickly grew to love its improvements to the RMarkdown workflow, like ease of rendering to different formats and intuitive structure of the index and YAML files. This was also when I received my first moment of creative liberty with the project, as I was tasked with dividing the ungrouped set of questions into discrete passages consisting of a stem and one or more corresponding items. Each of these passages–26 at the time–was given a title to identify it in the Quarto book sidebar.\nAt this point, the team came to a verdict on my initial pull request proposals: they approved and merged the slight text modification to “the majority” and the “fill-in-the-blank” title, but rejected the confidence interval language, asserting that the stem already provided sufficient motivation. The final step before presenting the assessment externally was to improve reproducibility, as many figures were not being rendered with each update, but rather embedded as images. In a throwback to my former STA 313 days, I used the current images as guides to recreate a map of the US colored by region, movie-themed data tables, and pseudocode chunks. I had some slight HTML and CSS exposure in previous classes, but this new challenge of matching an existing format allowed me to expand my knowledge about web layout and styles.\nBy April 2022, concluding my first semester working on the project, we had a polished, reproducible, website-hosted prototype of the assessment ready to present externally and gather feedback.\n\n\nPhase 1: Faculty Interviews\nThe remainder of the time spent working on the assessment–April 2022 to February 2023–was spent gathering feedback via interviews, iteratively updating items, and removing weaker questions based on group discussions. We conducted a series of three think-aloud informational interviews with various faculty nationwide who teach or have taught introductory data science courses. Participants were recruited from a shortlist of Dr. Çetinkaya-Rundel’s data science assessment contacts, and were specifically chosen to represent a breadth of data science curricula. The three interviewees chosen were, in order of interview: - Dr. Nicholas Horton (Professor of Statistics and Data Science at Amherst College) - Dr. Amelia McNamara (Assistant Professor of Computer and Information Science at the University of St. Thomas) - Dr. Kristin Stephens-Martinez (Assistant Professor of the Practice of Computer Science at Duke University)\nEach interview was scheduled for two hours long and consisted of three sections: open-ended introductory and concluding discussions, sandwiching an item-by-item run through of the assessment. In the initial discussion, we asked participants what they believed should be\n\nStats/DS Perspective\nhighlight a few things: one thing that hadn’t really thought about that Kristin pointed out, something we hadnt thoguht about that Amelia/Nick pointed out. one paragraph each, nothing crazy but just explaining what its like to get feedback\nhttps://docs.google.com/document/d/10_dGpgr2Dsj8BoWzvcvpUzwbtg842fl_hfOQgxMqvbs/edit\nhttps://docs.google.com/document/d/1s_0Z8WxQMO4qXblOnTjy4Wh7Ygq6kyD2OQGb0rfT9_U/edit\n\n\nCS Perspective\nhighlight a few things: one thing that hadn’t really thought about that Kristin pointed out, something we hadnt thoguht about that Amelia/Nick pointed out. one paragraph each, nothing crazy but just explaining what its like to get feedback\nhttps://docs.google.com/document/d/1edz9Erh2aB_RA9WsHPEqxm_dvOMvNkqj1BjmL73b-3A/edit\n\n\nRegrouping to synthesize feedback\nsummarising feedback from live sessions, working with the rest of the group to incorporate not buying lede in ethics questions, how to write questions that arent too obvious and get at specific things, language agnosticism (data entry in particular).\ntodo: keep going through google docs, im sure there are a bunch of big themes like the language agnosticism one that can be illustrated with particular items like that\n\n\n\nPhase 2: Student Interviews\nwith students with exposure: goal is to see if these topics are in your wheelhouse and if wording makes sense as a student balancing size and pacing\nthink about spacing out ethics quetsions to not prime–combatted question order suggestion (from AZ) early on\n\nRegrouping to synthesize feedback/final pilot assessment\nsummarising feedback from live sessions, working with the rest of the group to incorporate\n\n\nFinal assessment themes\nsomethign good would be good to have content: use 15 passages and assign them to rough buckets\nhttps://drive.google.com/drive/u/0/folders/1r_o_vhb0GakgQz9UpvUcnQRxKVVtriFf\n\n\n\nPhase 3: Large-scale Student Pilot\nfuture direction: students in 199 this spring would complete the assessment (either in full or a randomized subset of items) as an extra credit component of class. moving beyond individual item tweaking and refinement, this phase of the project will allow for real-world observations of pacing and length, feasibility for intro data science students\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26.\n\n\nDelmas, Robert, Joan Garfield, Ann Ooms, and Beth Chance. 2007. “ASSESSING STUDENTS’ CONCEPTUAL UNDERSTANDING AFTER A FIRST COURSE IN STATISTICS.” STATISTICS EDUCATION RESEARCH JOURNAL 6 (2): 28–58.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept Inventory-Measurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (8): 1018–27.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report. College Board.\n\n\nMulford, DouglasRobert. 1996. “An Inventory for Measuring College Students’ Level of Misconceptions in First Semester Chemistry.” Unpublished Master’s Thesis, Purdue University, IN.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7: 123–39.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” http://datascience.community/colleges.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16."
  },
  {
    "objectID": "04-instrument.html#quarto",
    "href": "04-instrument.html#quarto",
    "title": "Current Assessment Prototype",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "04-instrument.html#running-code",
    "href": "04-instrument.html#running-code",
    "title": "Current Assessment Prototype",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed).\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26.\n\n\nDelmas, Robert, Joan Garfield, Ann Ooms, and Beth Chance. 2007. “ASSESSING STUDENTS’ CONCEPTUAL UNDERSTANDING AFTER A FIRST COURSE IN STATISTICS.” STATISTICS EDUCATION RESEARCH JOURNAL 6 (2): 28–58.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept Inventory-Measurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (8): 1018–27.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report. College Board.\n\n\nMulford, DouglasRobert. 1996. “An Inventory for Measuring College Students’ Level of Misconceptions in First Semester Chemistry.” Unpublished Master’s Thesis, Purdue University, IN.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7: 123–39.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” http://datascience.community/colleges.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16."
  },
  {
    "objectID": "05-package.html#quarto",
    "href": "05-package.html#quarto",
    "title": "Package Development",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "05-package.html#running-code",
    "href": "05-package.html#running-code",
    "title": "Package Development",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed).\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26.\n\n\nDelmas, Robert, Joan Garfield, Ann Ooms, and Beth Chance. 2007. “ASSESSING STUDENTS’ CONCEPTUAL UNDERSTANDING AFTER A FIRST COURSE IN STATISTICS.” STATISTICS EDUCATION RESEARCH JOURNAL 6 (2): 28–58.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept Inventory-Measurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (8): 1018–27.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report. College Board.\n\n\nMulford, DouglasRobert. 1996. “An Inventory for Measuring College Students’ Level of Misconceptions in First Semester Chemistry.” Unpublished Master’s Thesis, Purdue University, IN.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7: 123–39.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” http://datascience.community/colleges.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16."
  },
  {
    "objectID": "06-discussion.html",
    "href": "06-discussion.html",
    "title": "Discussion",
    "section": "",
    "text": "also about future directions with 199 stuff\ntalk about how much quarto, html, css, yaml, qualtrics, github actions, CRAN etc ive learned\n\n\n\n\nÇetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look at Introductory Data Science.” Journal of Statistics and Data Science Education 29 (sup1): S16–26.\n\n\nDelmas, Robert, Joan Garfield, Ann Ooms, and Beth Chance. 2007. “ASSESSING STUDENTS’ CONCEPTUAL UNDERSTANDING AFTER A FIRST COURSE IN STATISTICS.” STATISTICS EDUCATION RESEARCH JOURNAL 6 (2): 28–58.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept Inventory-Measurement of the Effect of Teaching Methodology in Mathematics.” Notices of the American Mathematical Society 60 (8): 1018–27.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. Validating College Course Placement Decisions Based on CLEP Exam Scores: CLEP Placement Validity Study Results. Statistical Report. College Board.\n\n\nMulford, DouglasRobert. 1996. “An Inventory for Measuring College Students’ Level of Misconceptions in First Semester Chemistry.” Unpublished Master’s Thesis, Purdue University, IN.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel, Regina F. Frey, and Paul S. Mattson. 2021. “Development and Validation of an Introductory Psychology Knowledge Inventory.” Scholarship of Teaching and Learning in Psychology 7: 123–39.\n\n\nSwanstrom, Ryan. n.d. “Data Science Colleges and Universities.” http://datascience.community/colleges.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is Data Science? An Operational Definition Based on Text Mining of Data Science Curricula.” Journal of Behavioral Data Science 1 (1): 1–16."
  },
  {
    "objectID": "99-bibliography.html",
    "href": "99-bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Çetinkaya-Rundel, Mine, and Victoria Ellison. 2021. “A Fresh Look\nat Introductory Data Science.” Journal of Statistics and Data\nScience Education 29 (sup1): S16–26.\n\n\nDelmas, Robert, Joan Garfield, Ann Ooms, and Beth Chance. 2007.\n“ASSESSING STUDENTS’\nCONCEPTUAL UNDERSTANDING AFTER\nA FIRST COURSE IN\nSTATISTICS.” STATISTICS EDUCATION RESEARCH\nJOURNAL 6 (2): 28–58.\n\n\nEpstein, Jerome. 2013. “The Calculus Concept Inventory-Measurement\nof the Effect of Teaching Methodology in Mathematics.”\nNotices of the American Mathematical Society 60 (8): 1018–27.\n\n\nGodfrey, Kelly E., and Sanja Jagesic. 2016. Validating\nCollege Course Placement\nDecisions Based on CLEP\nExam Scores: CLEP\nPlacement Validity Study\nResults. Statistical Report.\nCollege Board.\n\n\nMulford, DouglasRobert. 1996. “An Inventory for Measuring College\nStudents’ Level of Misconceptions in First Semester Chemistry.”\nUnpublished Master’s Thesis, Purdue University, IN.\n\n\nSolomon, Erin D., Julie M. Bugg, Shaina F. Rowell, Mark A. McDaniel,\nRegina F. Frey, and Paul S. Mattson. 2021. “Development and\nValidation of an Introductory Psychology Knowledge Inventory.”\nScholarship of Teaching and Learning in Psychology 7: 123–39.\n\n\nSwanstrom, Ryan. n.d. “Data Science\nColleges and Universities.” http://datascience.community/colleges.\n\n\nZhang, Zhiyong, and Danyang Zhang. 2021. “What Is\nData Science? An\nOperational Definition Based on\nText Mining of Data\nScience Curricula.” Journal of\nBehavioral Data Science 1 (1): 1–16."
  }
]