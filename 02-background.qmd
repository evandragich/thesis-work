# Assessment Background {.unnumbered}

## Background

An essential component of any educational research is a validated, relevant instrument to measure students' learning outcomes. Whether to award college credit like the College Board's AP and CLEP exams, to measure students' previously-held misconceptions, or just as a tool upon which to evaluate educational interventions, such assessments have been developed and statistically analyzed for a wide range of subjects such as Spanish, Psychology, Chemistry, and Calculus [@godfrey2016; @solomon2021; @mulford2002; @epstein2013].

In the field of statistics, previous work on measuring students' reasoning skills led to the development of the Comprehensive Assessment of Outcomes in Statistics (CAOS). The revised CAOS 4, comprising 40 multiple-choice items on a variety of commonly-taught first-semester introductory concepts, was first administered in 2005 and allowed instructors to measure whether their courses were successfully resulting in their desired learning outcomes. However, many instructors noted that their findings reflected a much lower understanding than expected, specifically regarding the topics of data visualization and data collection [@delmas2007]. Notably, a key feature of the CAOS was the lack of hard computation nor need to recall specific formulas or definitions, allowing greater accessibility for a variety of statistics-adjacent uses. In fact, while initially motivated for research instrument purposes, early pilots of the CAOS found that instructors used the assessment results "for a variety of purposes, namely, to assign a grade in the course, for review before a course exam, or to assign extra credit" [@delmas2007].

In 2022, as more high schools and universities begin to support the emerging field of data science via specific courses, concentrations, or even majors, there is a need to measure students' learning outcomes in these introductory classes analogously to the subjects named above [@swanstrom; @schanzer2022]. Lacking a clearly-defined scope, empirical studies of so called "data science" curricula suggest that the field can be thought of an augmentation of traditional statistical modeling concepts, with emphases on computing, data visualization and manipulation, as well as a consideration of ethics and the role data plays in society [@zhang2021].

Specifically, a review of five introductory data science courses found that, while choice of language varied, all curricula involved some amount of computing or psuedocode [@çetinkaya-rundel2021]. The next highest frequency topics among curricula were inference and modeling, closely followed by data visualization and data wrangling, with most courses also having some component of communication and ethics. This empirical set of topics corroborates theoretical results from an earlier, larger conference of 25 data science-adjacent faculty, which identified six key competencies for undergraduate data science majors--computational thinking, mathematical fondations, model building and assessment, algorithms and software foundation, data curation, and communication [@deveaux2017].

Thus motivates the need for a language-agnostic, broad-scope data science assessment that can be tailored further to best meet the needs of specific programs. Given the breadth of diversity captured in the five curricula outlined in @çetinkaya-rundel2021, collaborating with a group of data science faculty to write such an assessment allows for a wide array of subjects to be covered, while still letting each member develop questions based on material they personally teach. In order to let items best model students' thinking processes and measure common misconceptions, think-aloud interviews with students are essential, not just to clarify potentially confusing wording, but also to ensure that students respond to each item via the through process intended by the researchers [@reinhart2022].

*transition into this next part by being like "the methodology doesnt stop there. after data colllected, we must analyze it for subscales and stuff, cite either delmas 2007 or find something else about validating scales* at some point acknowledge that were not finishing with just my theiss (analyzing student data, subscale stuff that wont be part of my thesis)
